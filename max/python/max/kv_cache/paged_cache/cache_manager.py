# ===----------------------------------------------------------------------=== #
# Copyright (c) 2025, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

from __future__ import annotations

import logging
from collections.abc import Sequence
from statistics import mean
from typing import Any

import numpy as np
from max.driver import Device, Tensor
from max.dtype import DType
from max.engine import InferenceSession
from max.graph import DeviceRef, Graph, TensorType, TensorValue
from max.interfaces import RequestID, TextGenerationContext
from max.nn.kv_cache import KVCacheParams, RaggedKVCacheInputs
from max.nn.kv_cache.data_parallelism_utils import (
    split_input_row_offsets,
    split_into_groups,
)
from max.nn.kv_cache.metrics import KVCacheMetrics
from max.profiler import traced

from .tp_cache_manager import _TPPagedKVCacheManager

logger = logging.getLogger("max.pipelines")


class PagedKVCacheManager:
    """Paged KVCache manager with data and tensor parallelism support.

    .. code-block:: python

        # Allocate metadata for requests in batch
        kv_manager.claim(ctx1.request_id, replica_idx=0)
        kv_manager.claim(ctx2.request_id, replica_idx=1)

        # Allocate blocks for these requests
        kv_manager.alloc(ctx1, num_steps=10)
        kv_manager.alloc(ctx2, num_steps=10)

        # Get KVCache inputs to feed to graph
        kv_cache_inputs = kv_manager.get_runtime_inputs([ctx1, ctx2], num_steps=10)

        # Run model...
        # Update requests with newly generated tokens
        ctx1.update(42)
        ctx2.update(42)

        # Commit newly written blocks to prefix cache
        kv_manager.step([ctx1, ctx2])

        # Release metadata and KV blocks for these requests
        kv_manager.release(ctx1.request_id)
        kv_manager.release(ctx2.request_id)
    """

    def __init__(
        self,
        params: KVCacheParams,
        session: InferenceSession,
        total_num_pages: int,
        total_num_host_pages: int = 0,
        enable_runtime_checks: bool = False,
    ) -> None:
        """Initialize the multi-device paged KV cache manager.

        Args:
            params: KV cache parameters including data parallelism settings
            session: The MAX Engine inference session
            total_num_pages: The total number of pages to allocate
            total_num_host_pages: The total number of host pages to allocate
            enable_runtime_checks: Whether to enable runtime checks
        """
        self.params = params
        self.devices = [d.to_device() for d in params.devices]

        self.num_replicas = params.data_parallel_degree
        assert len(self.devices) % self.num_replicas == 0, (
            "Number of devices must be divisible by number of replicas"
        )
        self.devices_per_replica = split_into_groups(
            self.devices, self.num_replicas
        )

        self._replica_managers: list[_TPPagedKVCacheManager] = []
        dp_1_params = params.copy_as_dp_1()
        for devices in self.devices_per_replica:
            self._replica_managers.append(
                _TPPagedKVCacheManager(
                    params=dp_1_params,
                    total_num_pages=total_num_pages,
                    total_num_host_pages=total_num_host_pages,
                    devices=devices,
                    session=session,
                    enable_runtime_checks=enable_runtime_checks,
                )
            )

        first_replica = self._replica_managers[0]
        self.page_size = first_replica.page_size
        self.enable_prefix_caching = first_replica.enable_prefix_caching
        self.enable_kvcache_swapping_to_host = (
            first_replica.enable_kvcache_swapping_to_host
        )
        self.total_num_pages = sum(
            manager.total_num_pages for manager in self._replica_managers
        )

        # Track requests to replicas.
        self._request_to_replica_idx: dict[RequestID, int] = {}
        self._request_count_per_replica: list[int] = [0] * self.num_replicas

        # Store session for model loading
        self.session = session

        # Initialize the ragged increment cache lengths model
        self.increment_cache_lengths_model = session.load(
            self._create_ragged_increment_cache_lengths_graph()
        )

    def get_replica(self, request_id: RequestID) -> int:
        return self._request_to_replica_idx[request_id]

    def get_or_recommend_replica(self, context: TextGenerationContext) -> int:
        """Return idx of the replica that should be used for the given request."""
        if context.request_id in self._request_to_replica_idx:
            return self._request_to_replica_idx[context.request_id]

        # Choose the replica with the fewest requests.
        replica_idx = min(
            range(len(self._request_count_per_replica)),
            key=self._request_count_per_replica.__getitem__,
        )
        return replica_idx

    def get_pct_used_blocks_after_allocation(
        self, ctx: TextGenerationContext, num_steps: int = 1
    ) -> float:
        """Get the percentage of blocks used after allocating for a request.

        Args:
            ctx: The request context containing sequence information and token indices.
            num_steps: Number of additional steps to allocate blocks for. Defaults to 1.

        Returns:
            The percentage of total blocks used after allocating for the request.
        """
        return self._replica_managers[
            self._request_to_replica_idx[ctx.request_id]
        ].get_pct_used_blocks_after_allocation(ctx, num_steps)

    def alloc(
        self,
        data: TextGenerationContext,
        num_steps: int = 1,
    ) -> None:
        """Allocates blocks for a request to run for N steps.

        This method allocates blocks needed by a request to run for N steps.
        When prefix caching is enabled, some of the allocated blocks may be
        retrieved from the prefix cache.

        Args:
            data: The text generation context for the request. The request ID
                must already be assigned to a replica via `claim`.
            num_steps: The number of steps to reserve blocks for. Default: 1.

        Raises:
            InsufficientBlocksError: If there are insufficient free blocks to
            satisfy the allocation.
        """
        assert data.request_id in self._request_to_replica_idx, (
            f"Request ID {data.request_id} must already be assigned to a "
            "replica before reserving"
        )
        replica_idx = self._request_to_replica_idx[data.request_id]
        return self._replica_managers[replica_idx].alloc(data, num_steps)

    def get_runtime_inputs(
        self, batch: Sequence[TextGenerationContext], num_steps: int = 1
    ) -> list[RaggedKVCacheInputs]:
        """Get the graph inputs for a batch of requests.

        This method will raise a RuntimeError if any request has insufficient blocks
        already allocated to it to run for the given number of steps.

        Args:
            batch: Batch of requests
            num_steps: Number of steps to run for
        """

        batch_by_replica: list[list[TextGenerationContext]] = [
            [] for _ in range(len(self.devices_per_replica))
        ]

        for ctx in batch:
            replica_idx = self._request_to_replica_idx[ctx.request_id]
            batch_by_replica[replica_idx].append(ctx)

        ret_list: list[RaggedKVCacheInputs] = []
        for replica_idx, ctxs in enumerate(batch_by_replica):
            ret_list.extend(
                self._replica_managers[replica_idx].get_runtime_inputs(
                    ctxs, num_steps
                )
            )
        return ret_list

    def release(self, request_id: RequestID) -> None:
        replica_idx = self._request_to_replica_idx.pop(request_id)
        self._request_count_per_replica[replica_idx] -= 1
        self._replica_managers[replica_idx].release(request_id)

    def claim(
        self, request_id: RequestID, replica_idx: int | None = None
    ) -> None:
        """Reserve a sequence ID for the given request ID."""
        if self.num_replicas > 1 and replica_idx is None:
            raise ValueError(
                "replica_idx must be specified when data parallelism is enabled"
            )
        if replica_idx is None:
            replica_idx = 0
        if request_id in self._request_to_replica_idx:
            raise ValueError(
                f"Request ID {request_id} is already claimed for replica {self._request_to_replica_idx[request_id]}"
            )
        self._replica_managers[replica_idx].claim(request_id)
        self._request_to_replica_idx[request_id] = replica_idx
        self._request_count_per_replica[replica_idx] += 1

    def step(self, batch: Sequence[TextGenerationContext]) -> None:
        for ctx in batch:
            replica_idx = self._request_to_replica_idx[ctx.request_id]
            self._replica_managers[replica_idx].step([ctx])

    def contains(self, request_id: RequestID) -> bool:
        return request_id in self._request_to_replica_idx

    @property
    def num_free_blocks(self) -> int:
        """Get the set of free blocks."""
        return sum(
            [manager.num_free_blocks for manager in self._replica_managers],
            start=0,
        )

    @property
    def metrics(self) -> KVCacheMetrics:
        return sum(
            (manager.metrics for manager in self._replica_managers),
            start=KVCacheMetrics(),
        )

    def reset_metrics(self) -> None:
        for manager in self._replica_managers:
            manager.reset_metrics()

    def _create_ragged_increment_cache_lengths_graph(self) -> Graph:
        input_symbols = self.params.get_symbolic_inputs()
        cache_lengths_types = [
            input_symbols[i][1] for i in range(len(self.devices))
        ]

        input_row_offsets_type = TensorType(
            DType.uint32,
            shape=["input_row_offsets_len"],
            device=DeviceRef(self.devices[0].label, self.devices[0].id),
        )

        data_parallel_splits_type = TensorType(
            DType.int64,
            shape=[self.params.data_parallel_degree + 1],
            device=DeviceRef.CPU(),
        )

        with Graph(
            "update_cache_lengths",
            input_types=[
                input_row_offsets_type,
                data_parallel_splits_type,
                *cache_lengths_types,
            ],
        ) as graph:
            inp_row_offset, data_parallel_splits, *cache_lengths = (
                inp.tensor for inp in graph.inputs
            )
            split_offsets = split_input_row_offsets(
                self.params.data_parallel_degree,
                inp_row_offset,
                data_parallel_splits,
            )
            outputs = []
            start_idx = 0
            for replica_idx in range(self.params.data_parallel_degree):
                devices = self.devices_per_replica[replica_idx]

                for i, device in enumerate(devices):
                    row_offset = split_offsets[replica_idx].to(
                        DeviceRef.from_device(device)
                    )
                    cache_length = cache_lengths[start_idx + i]
                    assert isinstance(cache_length, TensorValue)
                    right_slice = row_offset[1:].rebind(cache_length.shape)
                    left_slice = row_offset[: row_offset.shape[0] - 1].rebind(
                        cache_length.shape
                    )
                    increment_amount = right_slice - left_slice
                    outputs.append(cache_length + increment_amount)
                start_idx += len(devices)
            graph.output(*outputs)

        return graph

    @traced
    def increment_cache_lengths(
        self,
        kv_cache_inputs: Sequence[RaggedKVCacheInputs],
        prev_model_inputs: Any,
    ) -> Sequence[RaggedKVCacheInputs]:
        """Prepares cache inputs for the next token in multistep execution.

        Updates the cache lengths for the next inference step without requiring device
        synchronization or memory copies. This is crucial for maintaining performance
        during multi-token generation.

        Args:
            kv_cache_inputs: Current cache state tuples (blocks, lengths, lookup, max_lengths)
            prev_model_inputs: Previous model inputs including row offsets

        Returns:
            Updated cache input tuples with incremented lengths.
        """
        blocks = [cache_input.blocks for cache_input in kv_cache_inputs]
        cache_lengths = [cache_input.cache_lengths for cache_input in kv_cache_inputs]
        lookup_table = [cache_input.lookup_table for cache_input in kv_cache_inputs]

        if self.params.data_parallel_degree > 1:
            data_parallel_splits = prev_model_inputs.data_parallel_splits
        else:
            batch_size = cache_lengths[0].shape[0]
            data_parallel_splits = Tensor.from_numpy(
                np.array([0, batch_size], dtype=np.int64)
            )

        # Update the cache_lengths of our batch by the previous sequence length.
        # Handle both single tensor and list of tensors for compatibility
        if isinstance(prev_model_inputs.input_row_offsets, list):
            # InternVL case: use the first tensor (row offsets are identical across devices)
            row_offsets = prev_model_inputs.input_row_offsets[0]
        else:
            # Standard case: single tensor
            row_offsets = prev_model_inputs.input_row_offsets
        row_offsets = row_offsets.to(self.devices[0])

        updated_cache_lengths = self.increment_cache_lengths_model.execute(
            row_offsets, data_parallel_splits, *cache_lengths
        )

        start_idx = 0
        for devices in self.devices_per_replica:
            # max_lengths is host allocated and the same across each replica.
            max_lengths = kv_cache_inputs[start_idx].max_lengths

            # Advance to the next step of the max_lengths tensor.
            updated_max_lengths = max_lengths[1:, :]

            # Return our updated batch.
            assert isinstance(kv_cache_inputs, list)
            for i in range(len(devices)):
                updated_cache_length = updated_cache_lengths[start_idx + i]
                assert isinstance(updated_cache_length, Tensor)
                kv_cache_inputs[start_idx + i] = RaggedKVCacheInputs(
                    blocks=blocks[start_idx + i],
                    cache_lengths=updated_cache_length,
                    lookup_table=lookup_table[start_idx + i],
                    max_lengths=updated_max_lengths,
                )
            start_idx += len(devices)
        return kv_cache_inputs

    def reset_prefix_cache(self) -> None:
        for manager in self._replica_managers:
            manager.reset_prefix_cache()

    @classmethod
    def infer_optimal_batch_size(
        cls,
        params: KVCacheParams,
        max_seq_len: int,
        num_layers: int,
        available_cache_memory: int,
        devices: Sequence[Device],
        **kwargs: Any,
    ) -> int:
        # We just hard-code a default of 512 for paged attention.
        # The worst case scenario if this is too high is that we'll evict
        # requests at an elevated rate. We print warnings in that case so users
        # are aware of what needs to be tweaked/changed.
        return 512

    @property
    def free_blocks_pct(self) -> float:
        return mean(
            [manager.free_blocks_pct for manager in self._replica_managers],
        )

    @property
    def used_blocks_pct(self) -> float:
        return 1 - self.free_blocks_pct

    @property
    def host_committed_block_pct(self) -> float:
        return mean(
            [
                manager.host_committed_block_pct
                for manager in self._replica_managers
            ]
        )

    @property
    def total_num_host_pages(self) -> int:
        return sum(
            [manager.total_num_host_pages for manager in self._replica_managers]
        )

    def get_req_blocks(self, request_id: RequestID) -> list[int]:
        replica_idx = self._request_to_replica_idx[request_id]
        return self._replica_managers[replica_idx].block_manager.get_req_blocks(
            request_id
        )

    @property
    def device_tensors(self) -> list[list[Tensor]]:
        return [manager.device_tensors for manager in self._replica_managers]
